[{"categories":null,"content":"Understanding Kubernetes in simple terms.","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Kubernetes, often affectionately abbreviated as “K8s,” is one of the most buzzworthy technologies in the world of IT and software development. But for many people, especially those without a technical background, it can seem like an impenetrable wall of jargon and complexity. This article aims to break down Kubernetes into simple, relatable concepts that anyone can understand. By the end, you’ll not only know what Kubernetes is but also why it matters and how it works. Think of this as your friendly guide to understanding one of the most transformative tools in modern computing. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:0:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"The Problem Kubernetes Solves: Why Do We Need It? Before diving into what Kubernetes is, it’s essential to understand the problem it was designed to solve. Imagine you’re running a business with a website or an app that serves thousands—or even millions—of users. Behind the scenes, your app is powered by code running on servers. But here’s the catch: managing these servers and ensuring your app runs smoothly is no small feat. Traditionally, apps were built as single monolithic units that ran on individual servers. If something went wrong with the server—say it crashed or couldn’t handle a sudden spike in traffic—your app would go down, leaving users frustrated. Scaling up to meet demand meant buying more servers, which could be expensive and inefficient. Enter containers, a game-changer in how applications are deployed and managed. Containers package an application and all its dependencies into a lightweight, portable unit that can run consistently across different environments. Think of containers as shipping containers for software: they standardize how apps are transported and run, regardless of where they are deployed. But here’s the twist: managing containers at scale isn’t easy either. Imagine trying to juggle hundreds or thousands of containers across multiple servers while ensuring they communicate seamlessly, scale up or down based on demand, and recover from failures automatically. This is where Kubernetes comes in—it orchestrates all these moving parts so you don’t have to. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:1:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"What Is Kubernetes? An Everyday Analogy At its core, Kubernetes is an orchestrator for containers. But what does that mean? Let’s use an analogy. Imagine you’re running a busy restaurant kitchen. You have multiple chefs (servers) cooking various dishes (containers). Each dish requires specific ingredients (resources) and needs to be prepared in a particular order (workflow). As the restaurant owner, you want to ensure: Dishes are prepared on time. Chefs aren’t overworked or sitting idle. Ingredients are used efficiently. If a chef burns a dish or calls in sick, another chef steps in seamlessly. Now replace “chefs” with servers, “dishes” with applications running in containers, and “you” with Kubernetes. Kubernetes acts as the head chef or conductor, ensuring everything runs smoothly in your kitchen—or in this case, your IT infrastructure. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:2:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Core Concepts of Kubernetes Explained Simply To understand how Kubernetes works, let’s break down its key components using simple terms. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"1. Clusters: The Big Picture A cluster is like a team working together to get things done. In Kubernetes, a cluster consists of multiple machines (called nodes) that work together to run your applications. These machines can be physical computers or virtual servers in the cloud. The cluster has two main parts: Control Plane: The brain of the operation that makes decisions about what needs to be done. Worker Nodes: The hands that do the actual work by running your applications. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:1","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"2. Nodes: The Workers Each node is like an individual employee in your team. Nodes are responsible for running “pods,” which we’ll explain shortly. They have their own resources (CPU, memory, etc.) and follow instructions from the control plane. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:2","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"3. Pods: The Smallest Unit A pod is like a table at your restaurant where dishes (containers) are served together because they complement each other. For example: One container might serve as the main course (your app’s backend). Another container might serve as dessert (a database). Pods ensure these related containers share resources and communicate effectively. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:3","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"4. Control Plane: The Brain The control plane oversees everything happening in the cluster. It has several components: API Server: Acts as the receptionist—you interact with it to give instructions. Scheduler: Decides which node should handle which pod based on available resources. Controller Manager: Ensures everything stays on track; if something fails, it fixes it. etcd: A database that keeps track of what’s happening in the cluster. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:4","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"5. Services: Connecting Everything A service ensures that pods can talk to each other and users can access your app without worrying about where it’s running. It’s like giving each table at your restaurant a number so waiters know where to deliver food. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:5","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"6. Deployments: The Blueprint A deployment is like a recipe—it specifies how many replicas of a pod you need and ensures they stay up-to-date. For example, if you want three instances of your app running at all times, the deployment makes sure this happens automatically. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:3:6","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Why Is Kubernetes So Popular? Kubernetes has become incredibly popular because it solves real-world problems for businesses of all sizes. Here are some reasons why it’s such a big deal: Scalability: Kubernetes can automatically scale your app up or down based on demand. Resilience: If something goes wrong—like a server crashing—Kubernetes automatically recovers by restarting pods or moving them to healthy nodes. Portability: You can run Kubernetes on any infrastructure—whether it’s your local data center or a cloud provider like AWS or Google Cloud. Cost Efficiency: By optimizing resource usage, Kubernetes helps businesses save money on infrastructure costs. Automation: It takes care of repetitive tasks like deploying updates or balancing workloads across servers. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:4:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Real-Life Examples of Kubernetes in Action To make this even more relatable, let’s look at some real-life scenarios where Kubernetes shines: E-commerce Websites Imagine an online store preparing for Black Friday sales. Traffic spikes dramatically during this period, but you don’t want to overpay for unused servers during quieter times. Kubernetes can scale up resources during high traffic and scale them down afterward automatically. Streaming Services For platforms like Netflix or Spotify, user demand fluctuates throughout the day. Kubernetes ensures these services run smoothly by distributing workloads across multiple servers and recovering from failures instantly. Healthcare Apps In healthcare, apps must remain available 24/7 because lives depend on them. Kubernetes provides high availability by replicating critical services across multiple nodes. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:5:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Challenges with Kubernetes (and How They’re Being Addressed) While Kubernetes offers many benefits, it’s not without challenges: Complexity: Setting up and managing Kubernetes requires expertise. Learning Curve: Understanding its concepts can be daunting for newcomers. Resource Management: Poorly configured clusters can lead to inefficiencies. Fortunately, managed services like Google Kubernetes Engine (GKE) and Amazon Elastic Kubernetes Service (EKS) simplify much of this complexity by handling setup and maintenance for you. ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:6:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Conclusion Kubernetes might seem intimidating at first glance, but at its heart, it’s just a tool designed to make life easier for developers and businesses alike by automating the management of containerized applications. Whether you’re running a small startup or a global enterprise, Kubernetes provides the scalability, resilience, and flexibility needed to thrive in today’s fast-paced digital world. So next time someone mentions “K8s,” you’ll know they’re talking about one of the most powerful tools for orchestrating modern applications—and you’ll have some fun analogies to explain it! ","date":"2025-03-31","objectID":"/posts/2025/kubernetes-demystified-for-non-techies/:7:0","tags":["containers","kubernetes","devsecops"],"title":"Kubernetes Demystified: A Fun and Accessible Guide for Non-Techies","uri":"/posts/2025/kubernetes-demystified-for-non-techies/"},{"categories":null,"content":"Overview: Providing a framework for selecting the most suitable tools based on specific government needs.","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency: Because Security Shouldn’t Be an Afterthought DevSecOps has become a critical approach for federal agencies striving to deliver secure software at the speed of relevance. By March 2025, many agencies have made significant progress in their DevSecOps journey, but selecting the right tools remains a complex challenge that can make even seasoned SES members reach for the aspirin bottle. This guide provides a comprehensive framework for evaluating and selecting DevSecOps tools that align with your agency’s specific mission requirements, compliance needs, and existing infrastructure. Whether you’re supporting warfighters, processing benefits claims, or managing critical infrastructure, this guide will help you navigate the sea of vendor promises and shiny features to find solutions that actually work in government environments. ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:0:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"The Current State of DevSecOps in Government DevSecOps integrates security tools and practices into the development pipeline, emphasizes the automation of processes, and fosters a culture of collaboration between previously siloed teams. Since the release of key directives like the DoD Enterprise DevSecOps Strategy Guide in 2021 and President Biden’s Executive Order on Improving the Nation’s Cybersecurity, government organizations have been actively shoring up and standardizing their DevSecOps processes. However, government agencies face unique challenges: Legacy systems that weren’t designed for modern development practices Strict compliance requirements and Authorization to Operate (ATO) processes Limited access to commercial tools due to procurement constraints Classification boundaries that complicate tool deployment A workforce still transitioning to DevSecOps culture As one DevSecOps practitioner at a federal agency put it, “We wanted to shift left, but our process was so complicated that we ended up going in circles.” ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:1:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Understanding the DevSecOps Ecosystem ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:2:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"The Foundation: DevSecOps Reference Architectures The Department of Defense has established reference architectures that define how DevSecOps should be implemented in government settings. The DoD Enterprise DevSecOps Strategy Guide established strategic principles that every approved enterprise-wide DevSecOps reference design must support. A DevSecOps ecosystem consists of: Software Factories: Environments that combine tools and processes to enable the rapid development, testing, and deployment of secure applications CI/CD Pipelines: Automated workflows that move code from development through testing and into production Security Controls: Integrated checkpoints that evaluate code, configurations, and infrastructure for vulnerabilities Governance Frameworks: Processes that ensure compliance with organizational policies and government regulations ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:2:1","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"The Three Pillars of DevSecOps When selecting tools, consider how they support the three pillars of SecOps: Prevent: Tools that help prevent vulnerabilities from entering the codebase Resolve: Capabilities to discover and remediate security issues efficiently Secure: Features that demonstrate security compliance and posture ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:2:2","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Key Criteria for Evaluating DevSecOps Tools ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"1. Application Hardening Capabilities Tools should identify weaknesses in code and address them directly, including: Removing unnecessary code or software packages Eliminating sample or default sensitive files Disabling unused software features Identifying potential attack vectors ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:1","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"2. Repository-Level Protection Evaluate tools based on their ability to secure your code repositories through: Access controls that align with government requirements Vulnerability scanning before code is committed Secrets management to prevent credential exposure Branch protection policies ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:2","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"3. Environment Security Your DevSecOps toolchain should protect the environments where applications run by supporting: Infrastructure as Code (IaC) security scanning Configuration as Code (CaC) validation Container security (especially important for DoD Iron Bank compliance) Runtime protection capabilities ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:3","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"4. Integration with Development Planning Tools The tools should seamlessly integrate with existing planning and management systems to: Enable security requirements to be tracked alongside features Support documentation of security decisions Facilitate communication about security issues Allow for security metrics to be captured and reported ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:4","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"5. Automation Capabilities Automation is critical for embedding security throughout the development lifecycle: Static application security testing (SAST) automation Dynamic application security testing (DAST) integration Software composition analysis (SCA) for dependencies Infrastructure scanning automation ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:5","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"6. Compliance Support Government agencies have unique compliance requirements: Authority to Operate (ATO) support Continuous Authority to Operate (cATO) enablement Documentation generation for compliance evidence Mapping to NIST frameworks and DoD security controls ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:6","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"7. Government-Specific Considerations Beyond general criteria, government agencies should evaluate: FedRAMP authorization status Support for classified environments if needed Alignment with DoD Container Hardening requirements Compatibility with existing government platforms (e.g., Platform One) ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:3:7","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"The DevSecOps Tool Selection Framework ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:4:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Step 1: Assess Your Agency’s DevSecOps Maturity Before selecting tools, honest assessment is critical. One DoD official quipped, “We thought we were doing DevSecOps because we had a Jenkins server. Turns out we were just doing ‘Dev’ with occasional ‘Sec’ and hoping for ‘Ops’.” Evaluate your current posture across: Culture and collaboration Automation capabilities Security integration Monitoring and feedback Deployment frequency ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:4:1","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Step 2: Identify Your Tool Requirements Based on your assessment, document requirements across these categories: Development Tooling: IDEs with security plugins Code repositories with security features Developer security training platforms Build and Test Tools: CI/CD platforms SAST, DAST, and SCA tools Test automation frameworks Operations and Deployment: Container orchestration Configuration management Infrastructure as Code (IaC) tools Security Monitoring: Security information and event management (SIEM) Security orchestration, automation, and response (SOAR) Runtime application self-protection (RASP) ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:4:2","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Step 3: Evaluate Tool Integration Capabilities The DoD Enterprise DevSecOps Reference Design emphasizes that tools must work together in an integrated ecosystem. Evaluate: API availability and documentation Integration with existing government platforms Support for standard protocols and formats Data sharing capabilities between tools ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:4:3","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Step 4: Conduct Proof of Concept Testing As one government DevSecOps engineer noted, “Vendor demos are like first dates—everybody looks their best. You need to move in together for a while to see the real picture.” For critical tools: Test in environments similar to production Involve both developers and security personnel Validate key use cases and workflows Assess performance under realistic conditions ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:4:4","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Step 5: Consider Acquisition Strategy Government procurement adds complexity to tool selection: Evaluate contract vehicles and procurement options Consider enterprise agreements vs. per-project licensing Assess long-term maintenance and support costs Determine training requirements and availability ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:4:5","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Government DevSecOps Tool Categories and Recommendations ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:5:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"1. CI/CD Pipeline Tools Considerations: Must support automated security testing integration Should enable configuration as code for pipeline definitions Need to support deployment to government cloud environments Government Use Cases: The U.S. Air Force’s Kessel Run program exemplifies a tightly run and innovative software factory using integrated CI/CD pipelines Platform One provides Big Bang, a DoD-hardened Kubernetes platform that incorporates CI/CD capabilities ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:5:1","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"2. Security Testing Tools Considerations: Must generate findings in formats compatible with government reporting requirements Should support risk scoring aligned with government frameworks Need to integrate with existing vulnerability management processes Government Use Cases: DoD DevSecOps Reference Design emphasizes security scanning at multiple stages of the pipeline Security tools must leverage cloud service provider (CSP) managed service capabilities where practicable ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:5:2","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"3. Infrastructure and Configuration Management Considerations: Must support Infrastructure as Code (IaC) security scanning Should enforce DoD hardening requirements Need to integrate with cloud access points for government clouds Government Use Cases: DoD has developed Configuration as Code (CaC) profiles for tools like Anchore to ensure secure containers Tools must support automation at multiple levels across develop, build, test, release, and deliver phases ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:5:3","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"4. Monitoring and Observability Considerations: Must meet government logging requirements Should support threat detection use cases Need to operate within classification boundaries Government Use Cases: DoD emphasizes integration with SIEM and SOAR capabilities throughout the DevSecOps lifecycle Tools should enable “fully automated risk management” with defined control gates for risk characterization ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:5:4","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Implementation Best Practices ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:6:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"1. Start Small, Scale Deliberately The DoD Enterprise DevSecOps Fundamentals assumes that agile/iterative practices have already been implemented before moving to DevSecOps. Begin with: A single application or system A motivated, cross-functional team Clear metrics for success Executive sponsorship ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:6:1","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"2. Invest in Training and Culture As one government CISO noted, “We spent millions on tools and pennies on training. Then wondered why nothing changed.” Focus on: DevSecOps training for all team members Security awareness for developers Development basics for security teams Collaborative exercises and war games ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:6:2","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"3. Leverage Government Resources Don’t reinvent the wheel when government resources exist: DoD’s Iron Bank for hardened containers Platform One’s Big Bang for Kubernetes platforms Defense Unicorns’ resources for government DevSecOps Existing reference architectures and documentation ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:6:3","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"4. Plan for Continuous Improvement DevSecOps is a journey, not a destination. Build in: Regular tool assessment and optimization Collection and analysis of pipeline metrics Feedback loops from operations to development Updates based on emerging threats and technologies ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:6:4","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"Conclusion: The Path Forward Selecting the right DevSecOps tools for your government agency is not just about technical features—it’s about enabling a culture where security is everyone’s responsibility while meeting the unique requirements of public service. As you evaluate tools, remember that the goal isn’t perfect security (which doesn’t exist) or perfect compliance (which is always evolving). The goal is to build a capability that delivers secure software at the speed of relevance to support your agency’s mission. Or as one DoD DevSecOps team put it in their mission statement: “We’re here to help the government build software that doesn’t suck… securely.” By following this framework, your agency can select tools that not only meet technical requirements but also support the cultural transformation that is at the heart of true DevSecOps adoption. Because in government, we don’t just need to move fast and break things—we need to move fast and secure things. ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:7:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"References While we have cited relevant sources throughout this guide, these documents provide comprehensive guidance for government DevSecOps implementations: DoD Enterprise DevSecOps Strategy Guide DoD Enterprise DevSecOps Fundamentals DevSecOps Fundamentals Guidebook: DevSecOps Tools \u0026 Activities DoD Enterprise DevSecOps Reference Design Software DT\u0026E in DevSecOps Guidebook Remember: The best tool isn’t always the one with the most features or the highest price tag—it’s the one that enables your teams to deliver secure capability to your users while meeting the unique requirements of government service. ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:8:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"About AlphaBravo AlphaBravo specializes in delivering container and Kubernetes-based solutions tailored for government agencies, facilitating the seamless integration of DevSecOps practices. Our expertise encompasses the deployment of secure, compliant, and scalable application environments, ensuring that security measures are embedded throughout the software development lifecycle. By leveraging our deep understanding of federal acquisition processes and stringent security requirements, AlphaBravo assists agencies in automating processes, enhancing collaboration between development, security, and operations teams, and maintaining adherence to regulatory standards. Additionally, AlphaBravo offers comprehensive DevSecOps training programs through their ABLabs platform, equipping government personnel with the necessary skills to effectively implement and manage DevSecOps tools and methodologies. This training ensures that teams are proficient in modern development practices, containerization, and orchestration technologies, thereby enhancing the agency’s capability to deliver secure and efficient software solutions. ","date":"2025-03-27","objectID":"/posts/2025/choosing-right-devsecops-tools-gov/:9:0","tags":["dod","containers","security","kubernetes","devsecops"],"title":"A Guide to Choosing the Right DevSecOps Tools for Your Government Agency","uri":"/posts/2025/choosing-right-devsecops-tools-gov/"},{"categories":null,"content":"In 2025, government DevSecOps is going full throttle—automating, securing, and regulating like a caffeinated compliance officer with a CI/CD pipeline.","date":"2025-03-26","objectID":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/","tags":["dod","chainguard","containers","security","kubernetes","devsecops","devops"],"title":"Top 5 Trends in Government Devsecops for 2025","uri":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/"},{"categories":null,"content":"BLUF Research suggests automation will enhance security and efficiency in government DevSecOps by 2025, focusing on vulnerability scanning and CI/CD pipelines, with GSA leading in automated change management. It seems likely that shift-left security will integrate early, reducing vulnerabilities in development, a trend gaining traction in government agencies like the DoD. The evidence leans toward cloud-native security becoming essential as agencies adopt cloud infrastructures, securing containers and microservices, with NASA as a key example. Zero trust integration is expected to strengthen, aligning with US government cybersecurity mandates for continuous verification, notably in the Department of Energy. Regulatory compliance in DevSecOps is likely to grow, ensuring adherence to US standards like FedRAMP, NIST, and HIPAA, crucial for government operations, with the IRS ensuring system integrity. ","date":"2025-03-26","objectID":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/:0:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops","devops"],"title":"Top 5 Trends in Government Devsecops for 2025","uri":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/"},{"categories":null,"content":"Introduction Government DevSecOps is evolving rapidly in 2025, driven by the need for secure, efficient, and compliant software development. These trends are shaping how US agencies deliver digital services, balancing innovation with security. Let’s explore each trend with technical depth and a touch of humor, keeping it accessible for all readers. ","date":"2025-03-26","objectID":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/:0:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops","devops"],"title":"Top 5 Trends in Government Devsecops for 2025","uri":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/"},{"categories":null,"content":"Trends in Detail Automation for Enhanced Security and Efficiency Automation is the superhero of DevSecOps, swooping in to save the day by automating mundane tasks like vulnerability scanning and code analysis. For government agencies, this means faster identification and remediation of security flaws, reducing the attack surface. Imagine a world where your code is scanned faster than you can say “bureaucratic red tape”! This trend includes integrating security into CI/CD pipelines, ensuring every deployment is as secure as Fort Knox. The GSA has been a pioneer, automating change management to govern their platforms effectively. Shift-Left Security Shift-left security is like bringing the security guard to the brainstorming session, not just the exit. It integrates security early in the development lifecycle, catching vulnerabilities before they become nightmares. For government, this means developers and security teams collaborate from day one, ensuring secure code from the start. The DoD, for instance, has embraced this, making security a team sport rather than a last-minute penalty shootout. Cloud-Native Security As government agencies migrate to the cloud, cloud-native security becomes the new frontier. This involves securing containers, microservices, and serverless architectures, which can be as tricky as herding cats in a digital storm. It’s crucial for scalability and compliance, with NASA leveraging cloud-native tech for data processing, ensuring their systems are as secure as a space shuttle launch. Zero Trust Integration Zero trust is the “trust no one” policy of cybersecurity, and in DevSecOps, it means verifying every access attempt, whether inside or outside the network. For government, this aligns with mandates like the Federal Zero Trust Strategy, reducing risks of breaches. The Department of Energy is implementing this, ensuring every user and device is continuously validated, like a digital bouncer at a VIP event. Regulatory Compliance in DevSecOps Government agencies must navigate a maze of US regulations, and DevSecOps is their compass. This trend focuses on embedding compliance checks into the pipeline, ensuring adherence to standards like FedRAMP, NIST, and HIPAA. It’s like having a compliance officer embedded in your code, waving a flag if anything goes awry. The IRS, for example, integrates these checks to maintain system integrity, making audits smoother than a buttered slide. ","date":"2025-03-26","objectID":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/:0:3","tags":["dod","chainguard","containers","security","kubernetes","devsecops","devops"],"title":"Top 5 Trends in Government Devsecops for 2025","uri":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/"},{"categories":null,"content":"Note: Detailed Analysis of Government DevSecOps Trends for 2025 Government DevSecOps is at a pivotal juncture in 2025, with trends shaping how agencies deliver secure, efficient, and compliant software. This analysis delves into the technical underpinnings, government-specific implementations, and humorous insights, ensuring a comprehensive understanding for both technical and lay audiences. Background and Context The current landscape, as of March 24, 2025, shows government agencies increasingly adopting DevSecOps to integrate security into development and operations, driven by digital transformation and cybersecurity mandates. The focus is on balancing speed, security, and compliance, with trends emerging from recent reports and agency practices. Detailed Trend Analysis Trend 1: Automation for Enhanced Security and Efficiency Automation is a cornerstone of DevSecOps, automating tasks like vulnerability scanning, code analysis, and testing to enhance security and efficiency. For government, this reduces the attack surface by identifying flaws early, aligning with CI/CD pipeline integration. The Practical DevSecOps article from December 2023 highlights automation as a standard practice, providing real-time visibility into security posture. The GSA’s DevSecOps guide, updated in 2019 but still relevant, emphasizes automated change management, reaching maturity levels where patching occurs without downtime (Level 4). This trend is crucial for government, given the scale and sensitivity of their operations, ensuring rapid response to threats. Aspect Details Automation Tasks Vulnerability scanning, code analysis, testing, continuous monitoring Government Example GSA automates change management for platform governance Impact Reduces attack surface, enhances efficiency, aligns with CI/CD pipelines Trend 2: Shift-Left Security Shift-left security integrates security early in the development lifecycle, from design to deployment, ensuring vulnerabilities are caught before they escalate. This approach fosters collaboration between developers and security teams, reducing costs and effort. The DoD has been a leader, as noted in the FedTech Magazine article from June 2021, emphasizing early security planning. This trend is vital for government, given the high stakes of their software, ensuring secure code from the start, like catching a typo before it becomes a headline. Aspect Details Integration Point Early in design and development phases Government Example DoD adopts shift-left for enhanced security planning Impact Reduces vulnerabilities, fosters collaboration, lowers remediation costs Trend 3: Cloud-Native Security With government agencies moving to cloud infrastructures, cloud-native security secures containers, microservices, and serverless architectures. This trend, highlighted in the Practical DevSecOps article, involves using cloud-native tools and collaborating with cloud architects for configuration and access control. NASA, as mentioned in general trends, leverages cloud-native tech for data processing, ensuring security in scalable environments. This is critical for government, given compliance needs and the shift to multi-cloud strategies, as seen in the SAIC survey from February 2023. Aspect Details Technologies Secured Containers, microservices, serverless computing Government Example NASA uses cloud-native for data processing security Impact Ensures scalability, compliance, and security in cloud environments Trend 4: Zero Trust Integration Zero trust, a model of “never trust, always verify,” integrates into DevSecOps by continuously validating access attempts, aligning with government cybersecurity mandates. The Federal Zero Trust Strategy, outlined in the OMB memorandum from January 2022, drives this trend, with the Department of Energy implementing it for network security. The Appgate blog from September 2021 discusses how zero trust powers DevSecOps agility, reducing attack surfaces. This is essential for government, g","date":"2025-03-26","objectID":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/:0:4","tags":["dod","chainguard","containers","security","kubernetes","devsecops","devops"],"title":"Top 5 Trends in Government Devsecops for 2025","uri":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/"},{"categories":null,"content":"AlphaBravo: Empowering Government DevSecOps in 2025 AlphaBravo, a trailblazer in DevSecOps solutions (check us out at https://alphabravo.io), is perfectly poised to help government agencies tackle the top trends shaping Government DevSecOps in 2025. With our cutting-edge tools and expertise, we turn the daunting task of securing complex systems into a streamlined process that’s almost magical. Take automation, for example—AlphaBravo’s suite of tools swoops in like a digital superhero, automating vulnerability scanning and code analysis faster than you can say “bureaucratic bottleneck.” This allows government teams to detect and squash security threats in real-time, leaving outdated manual processes in the dust. Add in our shift-left security offerings—think hands-on training and consulting—and developers morph into security-savvy ninjas, catching vulnerabilities early in the development cycle before they can cause chaos. Cloud Security and Beyond As government agencies ride the cloud wave, AlphaBravo ensures the journey is secure with solutions tailored for cloud-native environments. Whether it’s locking down containers or safeguarding microservices, their tools provide Fort Knox-level protection—minus the gold bars, plus a hefty dose of encryption. Then there’s zero trust, a must-have in today’s cyber landscape. AlphaBravo’s frameworks make continuous verification feel like a well-oiled machine, scrutinizing every access request—trusted insider or sneaky outsider alike. And for the maze of regulatory compliance? Our expertise acts like a GPS, guiding agencies through standards without the usual headache. With AlphaBravo, the government doesn’t just keep up with 2025’s DevSecOps trends—they make it look secure, efficient, and dare we say, a little fun. ","date":"2025-03-26","objectID":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/:0:5","tags":["dod","chainguard","containers","security","kubernetes","devsecops","devops"],"title":"Top 5 Trends in Government Devsecops for 2025","uri":"/posts/2025/top-5-trends-in-government-devsecops-for-2025/"},{"categories":null,"content":"In 2025, government DevSecOps is going full throttle—automating, securing, and regulating like a caffeinated compliance officer with a CI/CD pipeline.","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide Before diving into the technical details, it’s important to note that securing Kubernetes in DoD and federal government environments requires adherence to specific standards and frameworks unique to these settings. This comprehensive guide provides a detailed roadmap for implementing security measures across different impact levels while ensuring compliance with government requirements. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:0:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Understanding DoD Impact Levels The foundation of any government Kubernetes security implementation begins with understanding the appropriate Impact Level (IL) classification for your data and applications, as this determines the security controls required. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:1:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Impact Level Classification Framework The DoD classifies cloud security by Impact Levels based on data sensitivity and potential impact if compromised: Impact Level 2 (IL2): Hosts publicly releasable or non-sensitive unclassified data Follows FedRAMP Moderate baseline requirements Allows minimal access controls (user ID/password authentication) Data can reside outside U.S. territories Impact Level 4 (IL4): Hosts Controlled Unclassified Information (CUI) and mission data Unauthorized disclosure could cause serious adverse effects Requires FedRAMP High baseline plus 38 additional DoD-specific controls Restricts access to U.S. Persons with proper background checks Systems must operate within U.S. territories Requires NIPRNET connectivity Impact Level 5 (IL5): Hosts higher-sensitivity CUI and Unclassified National Security Information (U-NSI) Adds nine additional controls beyond IL4 Restricts access to U.S. citizens only (stricter than IL4) Requires hardware token technology with multifactor authentication Mandates physical and logical separation of government data from non-federal entities Impact Level 6 (IL6): Handles classified information up to SECRET level Implements highest security controls Requires SIPRNet connectivity Restricts access to U.S. citizens with SECRET clearances Understanding these impact levels is essential as they dictate the security requirements your Kubernetes implementation must satisfy. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:1:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"DoD Kubernetes Reference Architectures DoD has developed specific reference architectures for Kubernetes deployments in government environments. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:2:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"DoD Enterprise DevSecOps Reference Design: CNCF Kubernetes This reference design outlines the key architectural components required for a compliant Kubernetes deployment: Cloud Native Access Point (CNAP): Manages all north-south network traffic within the infrastructure layer Conformant Kubernetes Installation: Must use properly certified Kubernetes implementations that have submitted conformance testing results to CNCF Centralized Artifact Repository: Must integrate with Iron Bank, the DoD Centralized Artifact Repository of hardened containers Service Mesh: Required within the Kubernetes orchestrator to manage all east-west network traffic Sidecar Container Security Stack (SCSS): Mandatory implementation to achieve zero-trust security down to the container/function level ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:2:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Multi-Cluster Kubernetes Reference Design For more complex environments, the Multi-Cluster Kubernetes Reference Design provides additional guidance: Immutable Infrastructure: Kubernetes clusters must be designed to be scaled up, destroyed, or replaced rather than modified in place Global Control Plane: Implements a global view across enclaves, allowing workloads to follow a clear path to production, even in disconnected or limited connectivity environments These reference architectures establish the foundation for secure Kubernetes deployments in government environments. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:2:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Kubernetes Cluster Hardening Cluster hardening in government environments typically follows the DISA STIG requirements and CIS Kubernetes Benchmarks. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:3:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"DISA Kubernetes STIG Requirements The DISA Kubernetes STIG version 1 release 11 contains 91 rules, categorized into different areas: Control Plane Configuration Critical control plane security requirements include: Disable anonymous authentication on the API server Disable token authentication to protect information in transit Disable Alpha APIs on the API server Enable ValidatingAdmissionWebhook Enable kernel protection in Kubelet Configure proper timeouts on Kubelet Audit Configuration Comprehensive audit logging is required for government clusters: Configure audit log path for the Kubernetes API Server Generate detailed audit records that capture event types, sources, results, users, and container associations Set appropriate audit log retention, size limits, and backup configurations End User Workload Security Workload security is equally important in government environments: Disable Kubernetes dashboard (unless explicitly required) Avoid using privileged host ports (below 1024) for user pods Never store secrets as environment variables (mount as volumes instead) Create and use dedicated namespaces for user-managed resources Implement Pod Security Standards to enforce baseline security controls ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:3:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Implementing CIS Kubernetes Benchmark The Center for Internet Security (CIS) Kubernetes Benchmark provides comprehensive security guidance for Kubernetes deployments: Layered Security Approach: Cluster-level security for physical infrastructure and configurable components Node-level security for host machine configurations Workload-level security for containers and applications Security Levels: Level 1: Basic security measures that don’t interfere with regular operations Level 2: Advanced security measures for deeper protection that may affect functionality Automated Compliance Verification: Use kube-bench to automatically verify compliance with CIS Benchmarks Integrate verification into CI/CD pipelines for continuous compliance # Example: Running kube-bench to verify CIS compliance docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest --version 1.18 ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:3:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Container Security for Government Environments Container security is particularly critical in government settings, with specific requirements for hardening and validation. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:4:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Iron Bank and Container Hardening The DoD’s Iron Bank provides hardened container images for government use: Key Components: Registry One: Fully compliant OCI registry with approved containers Repo One: Central repository for hardened container source code Vulnerability Assessment Tracker (VAT): Manages container justifications Iron Bank Front End (IBFE): Web interface for container catalog access Container Hardening Requirements: Containers are rebuilt and rescanned every 12 hours Images must comply with DoD security standards All vulnerabilities must be documented and justified ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:4:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"NIST SP 800-190 Guidance NIST’s Application Container Security Guide recommends specific practices: Host OS Optimization: Use container-specific host OSs with minimalist designs Disable unnecessary services to reduce attack surfaces Employ read-only file systems and other hardening practices Container Isolation: Run containers with the same purpose, sensitivity, and threat posture on a single host OS kernel Implement proper isolation between containers and from the host OS ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:4:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Network Security and Segmentation Network security in government Kubernetes deployments follows specific patterns based on impact level requirements. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:5:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Network Security by Impact Level Each impact level has specific network security requirements: IL4: Systems must operate within U.S. territories with NIPRNET connectivity IL5: Requires physical and logical separation of government data from non-government entities IL6: Mandates virtual/logical separation between DoD and federal tenants, with SIPRNet connectivity ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:5:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Kubernetes Network Security Within Kubernetes clusters, implement: Service Mesh Implementation: Mandatory for managing east-west traffic within the cluster Enables mTLS between services Enforces fine-grained access controls between microservices Network Policies: Define explicit ingress and egress rules for pods Implement default-deny policies and allow only necessary traffic Segment workloads by namespace with appropriate policies Cloud Native Access Point (CNAP): Implements zero-trust architecture for north-south traffic Provides access to development, testing, and production enclaves at different impact levels ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:5:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Access Control and Authentication Authentication and access control requirements vary by impact level and must be properly implemented. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:6:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Authentication Requirements by Impact Level IL2: Standard authentication following FedRAMP Moderate IL4: Enhanced authentication with background checks for personnel IL5: Hardware token MFA (DoDI 8520.03 Credential Strength D) or PKI certificates, restricted to U.S. citizens IL6: Highest security authentication methods, limited to U.S. citizens with SECRET clearances ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:6:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Role-Based Access Control (RBAC) RBAC is essential in government Kubernetes deployments: Principle of Least Privilege: Define roles with minimal permissions required for functions Create role bindings with appropriate scope (namespace or cluster) Regularly audit permissions to identify overly permissive roles Service Accounts: Use dedicated service accounts for each application Apply appropriate permissions to service accounts Regularly rotate service account tokens ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:6:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Secrets Management Secure handling of secrets is critical in government Kubernetes environments. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:7:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Approved Solutions HashiCorp Vault Enterprise stands out as a primary solution with FedRAMP compliance support: Vault Integration Methods: Vault Secrets Operator for native Kubernetes integration vault-k8s sidecar for injecting secrets into pods vault-csi-provider for mounting secrets as volumes Key Management: Implement automated key rotation Use FIPS 140-2 compliant encryption algorithms Implement proper access controls for key management ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:7:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Best Practices Follow DISA STIG requirements for secrets handling: Never store secrets as environment variables (always mount as volumes) Implement proper encryption of secrets at rest Establish audit logs for secrets access Use dedicated namespaces for secrets management ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:7:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Policy Enforcement and Runtime Security Policy enforcement ensures ongoing compliance with security requirements. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:8:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"OPA Gatekeeper Implementation Open Policy Agent (OPA) Gatekeeper enables enforcement of security policies: Policy Implementation: Define constraints for allowed/disallowed Kubernetes configurations Create constraint templates for reusable policies Implement audit and enforcement modes for policies Common Policy Areas: Repository restrictions for container images Resource quota requirements Label requirements for resources Security context constraints ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:8:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Google Policy Controller For GKE environments, Policy Controller offers pre-built policy bundles: Key Features: Integration with Google Cloud Pre-built policy bundles for security and compliance Support for custom constraint templates Policy Bundle Examples: Pod Security Policy replacements Cloud Service Mesh compliance General security best practices ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:8:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Continuous Monitoring and Vulnerability Management Continuous monitoring is essential for maintaining security posture over time. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:9:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Vulnerability Scanning Implement comprehensive vulnerability scanning: Container Scanning: Scan base images in CI/CD pipelines Perform regular scans of running containers Integrate with DoD vulnerability databases Infrastructure Scanning: Use tools like Checkov and Terrascan for IaC scanning Implement kube-bench for ongoing compliance verification Perform regular penetration testing ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:9:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Continuous Monitoring Establish monitoring systems for security events: Security Monitoring: Implement real-time monitoring of cluster activities Configure alerting for potential security incidents Integrate with security information and event management (SIEM) systems Audit Logging: Configure comprehensive audit logging per DISA STIG requirements Establish appropriate retention policies Implement secure log storage and analysis ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:9:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Implementation Recommendations by Impact Level The following sections provide specific implementation guidance for each impact level. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:10:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"IL2 Implementation (Non-CUI Information) For basic IL2 deployments: Baseline Requirements: Implement FedRAMP Moderate controls Follow CIS Kubernetes Benchmark Level 1 recommendations Use hardened container images from Iron Bank Authentication: Implement standard authentication methods Apply basic RBAC configurations ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:10:1","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"IL4 Implementation (CUI Data) For IL4 environments handling CUI: Enhanced Security: Implement FedRAMP High controls plus DoD-specific requirements Follow CIS Kubernetes Benchmark Level 2 recommendations Restrict access to U.S. Persons with appropriate background checks Network Security: Deploy within U.S. territories only Implement NIPRNET connectivity Apply comprehensive network policies ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:10:2","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"IL5 Implementation (Higher-Sensitivity CUI) For IL5 environments with sensitive national security information: Advanced Security: Implement all IL4 controls plus additional IL5-specific requirements Restrict access to U.S. citizens only Require hardware token MFA or PKI certificates Enhanced Segregation: Implement physical and logical separation from non-federal entities Apply strict pod security policies Implement comprehensive audit logging ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:10:3","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"IL6 Implementation (Classified Information) For IL6 environments with classified information: Maximum Security: Implement highest security controls Restrict access to cleared personnel Require SIPRNet connectivity Classified Data Handling: Apply strict data encryption requirements Implement enhanced monitoring and alerting Follow all classified information handling requirements ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:10:4","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Wrapping It Up Securing Kubernetes in government environments requires a comprehensive approach that addresses the unique requirements of DoD and federal systems. By following the specific guidance for each impact level and implementing the security controls outlined in DoD reference architectures, you can establish a secure foundation for container workloads in government settings. The key to success is understanding the applicable impact level for your environment and implementing the corresponding security controls systematically. Regular assessment against DISA STIGs and CIS Benchmarks ensures ongoing compliance, while integration with approved tools and solutions helps maintain security posture over time. As government security requirements evolve, staying current with DISA, NIST, and DoD guidance is essential for maintaining compliance and protecting sensitive information in Kubernetes environments. Make sure you checkout the latest DoD DevSecOps available on the DISA website: https://public.cyber.mil/devsecops/ ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:11:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"How AlphaBravo Can Accelerate Your Mission At AlphaBravo, we specialize in securing Kubernetes clusters for government and defense organizations through our battle-tested ABOps platform and deep compliance expertise. Our approach integrates DISA STIG enforcement and Zero-Trust Architecture directly into your Kubernetes operations, ensuring your clusters meet DoD Impact Level 4 (IL4) and IL5 requirements out of the box. We deploy pre-hardened, Iron Bank-compliant containers and automate security controls like network policy enforcement and runtime vulnerability scanning, eliminating configuration drift while maintaining ATO readiness. Our platform will enforce NIST 800-53 and CNCF best practices across multi-cloud and air-gapped environments, providing consistent security whether you’re operating on AWS GovCloud, Azure Government, or disconnected tactical networks. We solve unique government challenges through patented low-bandwidth GitOps synchronization and SBOM-driven artifact traceability – critical for environments with limited connectivity or strict software supply chain mandates. Our ABOps platform embeds security at every layer, from STIG-hardened host OS kernels to service mesh-enabled micro-segmentation that satisfies DoD’s zero-trust requirements. For classified workloads (IL6), we can implement NIPR/SIPR-compliant deployment patterns with FIPS 140-2 validated cryptographic modules and hardware-backed key management. Our team brings operational experience from supporting DoD Platform One and PEO IWS Forge, translating policy into actionable controls. We pair our ABOps platform with hands-on Kubernetes Advanced Training that teaches your team to mitigate CVE-flagged containers and conduct NIST SP 800-190 compliant cluster audits. Whether you’re migrating legacy systems to Rancher Government Solutions or hardening vanilla Kubernetes for Mission Partner Environments (MPE), we provide the expertise and technology you need. ","date":"2025-03-24","objectID":"/posts/2025/securing-k8s-federal-gov-tech-guide/:12:0","tags":["dod","chainguard","containers","security","kubernetes","devsecops"],"title":"Securing Kubernetes Clusters in Federal Government Environments: A Technical Guide","uri":"/posts/2025/securing-k8s-federal-gov-tech-guide/"},{"categories":null,"content":"Cloudflare doesn't let you bulk delete in the UI. In this blog, we show you how to use Python script to bulk delete DNS records in Cloudflare.","date":"2021-12-23","objectID":"/posts/2021/bulk-delete-clouflare-dns/","tags":["cloudflare","dns"],"title":"Bulk Delete Cloudflare DNS Records","uri":"/posts/2021/bulk-delete-clouflare-dns/"},{"categories":null,"content":"Cloudflare is a powerful platform that can help with numerous aspects of managing and securing your infrastructure. We will cover some of the other features in future blogs, but I highly recommend that you look into how Cloudflare can help you or your organization. In this blog we will discuss a mildly annoying feature of Cloudflare. You can’t easily delete DNS records in bulk. We rely on Cloudflare for our DNS record management. It allows you to protect the actual source IP behind the Cloudflare proxy network and enables DDOS and other protection (enabling the proxy is optional). We also use Cloudflare for many trainings, demos and general testing. Often, we create lots of records manually for demonstration purposes, but when it comes time to delete them, you have to delete them. One…. At…. A…. Time. Luckily, we can leverage a simple python script to delete the records in bulk. ","date":"2021-12-23","objectID":"/posts/2021/bulk-delete-clouflare-dns/:0:0","tags":["cloudflare","dns"],"title":"Bulk Delete Cloudflare DNS Records","uri":"/posts/2021/bulk-delete-clouflare-dns/"},{"categories":null,"content":"The Bulk Delete Script WARNING Bulk deleting DNS records is generally NOT advised. Cloudflare likely does not allow for it in the UI because you could seriously mess up your production infrastructure by bulk deleting records you shouldn’t. Make CERTAIN that you intend to delete ALL records in a Zone. Use this script at your own risk. YOU HAVE BEEN WARNED. Now that you have read the above warning, let’s look at the script. The Python script has a few components: zoneid: This field indicates the ID of the DNS zone you want to delete records from. You can get zone ID from Overview page on the bottom right section. bearer_token: This is an API key from your Cloudflare interface that allows you to modify DNS records. More information on creating this here Fetch Records: This section queries the specified DNS Zone for records Delete Records: This section actually deletes the record discovered in the Fetch section import json import requests # You can get zone ID from Overview page on the bottom right section zoneid = \"YOUR_ZONE_ID\" # You need to generate api token from My Profile \u003e API Token with Edit Zone permission bearer_token = \"YOUR_BEARER_TOKEN\" if input(\"Are you sure you want to delete ALL DNS records in this zone? (y/n)\") != \"y\": exit() # Fetch dns records from CloudFlare record_rq = requests.get(\"https://api.cloudflare.com/client/v4/zones/\"+ zoneid +\"/dns_records\", headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \" + bearer_token}) data = record_rq.json() if data['success'] == False: print(\"Failed to fetch dns record:\") print(data['errors']) quit() # Delete dns records for record in data['result']: print(\"Deleting:\", record['name']) rq = requests.delete(\"https://api.cloudflare.com/client/v4/zones/\"+ zoneid +\"/dns_records/\" + record['id'], headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \" + bearer_token}) print(rq.status_code, \"\\n\") ","date":"2021-12-23","objectID":"/posts/2021/bulk-delete-clouflare-dns/:1:0","tags":["cloudflare","dns"],"title":"Bulk Delete Cloudflare DNS Records","uri":"/posts/2021/bulk-delete-clouflare-dns/"},{"categories":null,"content":"Running The Script Create a new file on a system that has python3 installed. On a Mac or Linux system: vi cloudflare-bulk-delete-dns.py Past the contents from the above script into that file. Make sure you update the zoneid and bearer_token fields with your own specific information. Run the script with the following command after you have read the above warning. python3 cloudflare-bulk-delete-dns.py Wait for the script to delete all DNS records in that zone. ","date":"2021-12-23","objectID":"/posts/2021/bulk-delete-clouflare-dns/:2:0","tags":["cloudflare","dns"],"title":"Bulk Delete Cloudflare DNS Records","uri":"/posts/2021/bulk-delete-clouflare-dns/"},{"categories":null,"content":"Closing Hopefully this script saves you some time in the future when cleaning up DNS records. If you want help implementing Cloudflare DNS or other features for your enterprise, please reach out to us at info@alphbravo.io That’s all for now! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-12-23","objectID":"/posts/2021/bulk-delete-clouflare-dns/:3:0","tags":["cloudflare","dns"],"title":"Bulk Delete Cloudflare DNS Records","uri":"/posts/2021/bulk-delete-clouflare-dns/"},{"categories":null,"content":"We show you how to manage and work with multiple `kubeconfig` files from the comfort of the Linux CLI.","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Scaling beyond managing a single Kubernetes cluster from the CLI gets hard quickly. Even after digging in on the official documentation, we feel that the process is just too complex to use efficiently on a daily basis. Luckily we have some tips and tools that should help you with that process. ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:0:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"kubeconfig File Management Let’s start with managing our kubeconfig files. They need to be treated with care as they are literally the keys to your Kubernetes kingdom. Keep them SAFE! Store them in a secret vault or password manager. Lock them down with proper permissions on your systems. Don’t share them publically (or even privately). Update them to be meaningful. Usually the kubeconfig files are output using the name default for everything, which is fine for a single cluster but does not work when trying to manage multiple files/clusters. Organize them on your system so they can be reused again and again. Some folks merge their Kubeconfigs and toss the originals. I think being able to regenerate that merged kubeconfig on-demand from the original sources is important. ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:1:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Updating the fields in your kubeconfig The first thing we need to do is update the fields in each kubeconfig to be unique and meaningful. IMPORTANT Make copies of these before modifying them. (cp config config.orig) If you accidentally change something you shouldn’t have, the file will no longer work and you may lose access to that cluster until you generate a new file. Pay close attention to the relationship of the fields. The cluster and user fields relate to other fields in the file and should remain an equal value. Here is an example of a kubeconfig with the often found default values: apiVersion: v1 clusters: - cluster: certificate-authority-data: fake-ca-file server: https://0.0.0.0:6443 name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: fake-cert-file client-key-data: fake-key-file Below is an updated ‘kubeconfig’ with unique and descriptive: apiVersion: v1 clusters: - cluster: certificate-authority-data: fake-ca-file server: https://0.0.0.0:6443 name: prod-cluster contexts: - context: cluster: prod-cluster user: admin@prod-cluster name: prod-cluster current-context: prod-cluster kind: Config preferences: {} users: - name: admin@prod-cluster user: client-certificate-data: fake-cert-file client-key-data: fake-key-file Once the cleanup is complete on all files and they are given meaningful names (prod-cluster, dev-cluster, etc), move them into the ~/.kube/ directory on your system. ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:2:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Important Note Linux and BASH Specific The following instructions were tested on a Linux host (Ubuntu 20.04) and using the BASH shell. Pay close attention as your steps and requirements for implementing this may change based on if you are using another Linux distribution, MacOS, or Windows or are using another shell like ZSH or FISH. ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:3:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Welcome to Krew Now that you have your kubeconfig files all in order, let’s leverage some of the excellent kubectl tools that are out there. Kubernetes Krew “is the package manager for kubectl plugins.” You can get started by installing Krew here. Krew supports Windows, Mac and other Linux distributons and instructions for each system are in that link. I typically install it by running their included script on my system: ( set -x; cd \"$(mktemp -d)\" \u0026\u0026 OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" \u0026\u0026 ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" \u0026\u0026 KREW=\"krew-${OS}_${ARCH}\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" \u0026\u0026 tar zxvf \"${KREW}.tar.gz\" \u0026\u0026 ./\"${KREW}\" install krew ) If you are using BASH, add the path export to the ~/.bashrc file. You may need to check here on what to do if you are using FSH, ZSH or another shell. export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" Reload your shell with the new ~/.bashrc settings. source ~/.bash_profile source ~/.bashrc Check to make sure Krew is installed and you can see the version: kubectl krew version Output: OPTION VALUE GitTag v0.4.2 GitCommit 6fcdb79 IndexURI https://github.com/kubernetes-sigs/krew-index.git BasePath /root/.krew IndexPath /root/.krew/index/default InstallPath /root/.krew/store BinPath /root/.krew/bin DetectedPlatform linux/amd64 Now that we have Krew installed, we can install the specific plugins we are interested in for this excercise. ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:4:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Install kubectx and kubens with Krew The 2 plugins that we will use are kubectx and kubens. They can be installed standalone, but I prefer to use them as part of Krew. kubectx: A tool to switch between contexts (clusters) on kubectl faster. kubens : A tool to switch between Kubernetes namespaces (and configure them for kubectl) easily. You can follow the install instructions here. Run the following commands to install with KREW. kubectl krew install ctx kubectl krew install ns Krew also has numerous other useful plugins. To view them all run: kubectl krew search ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:5:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Install FZF (Optional) FZF is not a necessary plugin, but it is recommended by kubens and kubectx to make selections a bit more intuitive. Recommendation for FZF by the kubectx and kubens dev here. Install instructions for FZF here. git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf ~/.fzf/install Reload your shell with the new ~/.bashrc settings to make sure FZF is active. source ~/.bash_profile source ~/.bashrc ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:6:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Merging kubeconfig files This part is highly opinionated. This what works for me, but I am interested to hear if you know of a better / more efficient way! All kubeconfig files should be in your ~/.kube/ directory for this and properly named / sanitized (per above). root@rke2-test:~# ls ~/.kube -lah total 44K drwxr-xr-x 2 root root 4.0K Dec 9 18:24 . drwx------ 10 root root 4.0K Dec 8 18:26 .. -rw-r--r-- 1 root root 15K Dec 9 18:24 config -rw-r--r-- 1 root root 3.0K Dec 9 18:20 dev-cluster -rw-r--r-- 1 root root 3.0K Dec 9 18:17 prod-cluster -rw-r--r-- 1 root root 3.0K Dec 9 18:21 qa-cluster -rw------- 1 root root 3.0K Dec 9 18:13 rke2-homelab -rw-r--r-- 1 root root 3.0K Dec 9 18:21 test-cluster Next, we will use kubectl to merge all of the files we want (in this case, I use every kubeconfig file in the directory, but you can pick and choose) into a single file at ~/.kube/config. Since this is the default location where kubectl looks, we don’t need to update our KUBECONFIG variable anymore. KUBECONFIG=~/.kube/rke2-homelab:~/.kube/prod-cluster:~/.kube/dev-cluster:~/.kube/qa-cluster:~/.kube/test-cluster \\ kubectl config view --flatten \u003e tmp \u0026\u0026 mv tmp ~/.kube/config Now, if we were to look at our ~/.kube/config file, we would see all the separate files merged into one: (Click the arrow next to YAML to expand) apiVersion: v1 clusters: - cluster: certificate-authority-data: fake-ca-file server: https://127.0.0.3:6443 name: dev-cluster - cluster: certificate-authority-data: fake-ca-file server: https://127.0.0.2:6443 name: prod-cluster - cluster: certificate-authority-data: fake-ca-file server: https://127.0.0.4:6443 name: qa-cluster - cluster: certificate-authority-data: fake-ca-file server: https://127.0.0.1:6443 name: rke2-homelab - cluster: certificate-authority-data: fake-ca-file server: https://127.0.0.5:6443 name: test-cluster contexts: - context: cluster: dev-cluster user: dev-cluster name: dev-cluster - context: cluster: prod-cluster user: prod-cluster name: prod-cluster - context: cluster: qa-cluster user: qa-cluster name: qa-cluster - context: cluster: rke2-homelab user: rke2-homelab name: rke2-homelab - context: cluster: test-cluster user: test-cluster name: test-cluster current-context: rke2-homelab kind: Config preferences: {} users: - name: dev-cluster user: client-certificate-data: fake-cert-file client-key-data: fake-key-file - name: prod-cluster user: client-certificate-data: fake-cert-file client-key-data: fake-key-file - name: qa-cluster user: client-certificate-data: fake-cert-file client-key-data: fake-key-file - name: rke2-homelab user: client-certificate-data: fake-cert-file client-key-data: fake-key-file - name: test-cluster user: client-certificate-data: fake-cert-file client-key-data: fake-key-file ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:7:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Using kubectx and kubens Now that we have our files all merged, we can start easily switching contexts and namespaces with kubectx and kubens. To clarify their purpose: kubectx is used for switching contexts. This allows you to switch to managing different clusters. kubens is used for switching namespaces inside of each context. This allows for you to target specific namespaced resources inside an individual cluster. Switch to the prod-cluster: kubectl ctx prod-cluster # Switched to context \"prod-cluster\". Switch to the devcluster: kubectl ctx dev-cluster # Switched to context \"dev-cluster\". Get a selectable list of clusters you can switch to (if you installed FZF): kubectl ctx Switch to the cattle-system namespace on the prod-cluster: kubectl ctx prod-cluster kubectl ns cattle-system ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:8:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Closing Hopefully, this blog helps you get a handle on your ever-growing list of Kubernetes clusters that you are managing. We recommend using Rancher if you want a powerful Web UI to manage your clusters, but sometimes the CLI is necessary. If you have any questions or would like AlphaBravo’s assistance in building production-grade Kubernetes clusters, please reach out to us at info@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-12-08","objectID":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/:9:0","tags":["kubernetes","kubeconfig","krew"],"title":"Zero To Hero: Working With Multiple Kubeconfig Files","uri":"/posts/2021/zero-to-hero-working-with-multiple-kubeconfig/"},{"categories":null,"content":"Part 2 of the cheap single node Kubernetes series. In this blog we deploying Longhorn, Monitoring and Logging on RKE2.","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Single-Node Kubernetes Series Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1) Powerful Single Node K3S on Hetzner for CHEAP! (Part 1a) Powerful Single Node K3s on Hetzner for CHEAP! (Part 2) ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:1:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"RKE2 Install In the first blog of this series, we deployed a single node RKE2 deployment and Rancher UI or a cheap Hetzner server. In this post, we will install the following: Longhorn for Persistent Data Rancher Monitoring EFK Logging (Elasticsearch,Fluent Bit,Kibana) ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:2:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"What is Longhorn Longhorn is an official CNCF project that delivers a powerful cloud-native distributed storage platform for Kubernetes that can run anywhere. When combined with Rancher, Longhorn makes the deployment of highly-available, persistent, block storage in your Kubernetes environment easy, fast, and reliable. Some of the key features are: An Inuitive Dashboard Easy 1-Click Deployment Built in Disaster Recovery Tools Infrastructure Agnostic means it run on any Kubernetes, anywhere A CNCF Sandbox Project ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:3:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Longhorn Official Longhorn Documentation As noted in the features above, the install of Longhorn has really been simplified using the one-click installer in the Rancher UI “apps” section. Follow the instructions below to get started: In Rancher MCM, Navigate to the cluster where you will install Longhorn. Navigate to the Apps \u0026 Marketplace, Charts menu item. Find the Longhorn item in the charts and click it. Click Install. Customize the default settings by selecting Customize Helm options before install box. Under Longhorn Storage Class Settings change the Default Storage Class Replica Count from 3 to 1. This is because we are running a single server and Longhorn by default expects 3 separate servers with 3 copies of the data for redundancy. In an actual production cluster, you can leave this at 3. Click Next and Install and Longhorn will install on RKE2. Notes You may want to add additional volumes specifically for Longhorn. To do that, add a volume, format at EXT4 or XFS, and mount it on your host/s. Then you can point to that path in the Longhorn config. In this case, we are just using the additional space on the root volume of the disk as longhorn automaticall mounts to /var/lib/longhorn. You may need to install iscsi-initiator-utils on your system for Longhorn to install properly. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:4:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Accessing Longhorn Now that Longhorn is installed, we can access the Longhorn UI via the Rancher UI. In Rancher MCM, Navigate to the cluster where installed Longhorn. Navigate to the Longhorn menu item. Click Manage storage system via UI on the Longhorn card and a new window will open with your Longhorn UI showing. Longhorn UI on RKE2 ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:5:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Monitoring Now that we have Longhorn installed for Persistent Storage, we can install Monitoring with persistence. The Rancher Monitoring chart will install Prometheus, Grafana, Alert-Manager and other tools to monitor the cluster. Login to the Rancher UI. Go to Apps and Marketplace, Charts. Click on Monitoring menu item and click Install. Leave the version as the default, but select Customize Helm Options before Install at the bottom of the page. Click Next. Under Prometheus (on the inner left column), change the following options: Retention Size: 10GiB CPU Limit: 1500m Memory Limit 2048Mi Select the box for Persistent Storage for Prometheus. Size: 10Gi Storage Class Name: longhorn Under Grafana (on the inner left column), change the following options: Select Enable with PVC Template Size: 10Gi Storage Class Name: longhorn AccessMode: ReadWriteOnce Click Next, then click Install. Once this is complete, you should now have access to monitoring for your node and workloads running in your cluster. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:6:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Accessing Monitoring Monitoring is complex, especially when it comes to alert configuration.. To get you started, we will show you 2 ways to see what is being monitored and your cluster stats. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:7:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Via the Workloads In the Rancher UI, click on Workload, Pods. Click on a workload name like longhorn-manager-010101 and then click on Metrics. You can see the different metrics for that specific Pod and change the time scale you ae see it in. You can also click on Grafana to be taken to the full Granfana dashboard for those specific metrics. Workload Metrics UI ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:7:1","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Via Grafana In the Rancher UI, click on Monitoring in the menu. Under Grafana, click on Metrics Dashboard. This will open a new tab for Grafana The Welcome screen shows stats for the cluster as a whole. To explore the cluster more, hover on the 4 boxes in the menu and click Manage. This will show you an entire list of metrics you can drill down into and view. Grafana UI ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:7:2","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Logging Logging is a large scale endeavor in itself. Make sure you understand the needs of your organization and whether or not you should log to a hosted service (we like LogDNA). However, in many cases, sending logs externally is not an option. In today’s example, we will deploy Elasticsearch locally to ingest the logs, Kibana to view them, and Fluent-bit to ship them from the containers to Elastic. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Elasticsearch For Elasticsearch and Kibana, we will use the Helm charts maintained by Bitnami, a VMWare entity. They are very well maintained and well documented and are well suited for this application. Login to the Rancher Interface. Go to Apps and Marketplace, Repositories. Click Create and add the following then click Create again. Name: bitnami Index URL: https://charts.bitnami.com/bitnami Under Cluster, click Projects/Namespaces. Under the Default project, click Create Namespace and name it logging-system. Go to Apps and Marketplace, Charts. Select only bitnami from the chart repo options. Click on elasticsearch menu item and click Install. Update the following and click Next: Namespace: logging-system Name: elasticsearch-logging Customize Helm options before install: check. Replace all the YAML with the below code. In the below code, update the elasticPassword: with your desired password. Make this complex because it is complicated to change. Save this password as it will be required for Kibana and Fluent. Expand The Code In the next few sections, the code blocks are quite large. We are showing them and collapsed in to to make this doc easier to read. In order to view/copy the code, click the arrow next to YAML in each section to expand the code. clusterDomain: cluster.local config: {} coordinating: affinity: {} autoscaling: enabled: false maxReplicas: 3 minReplicas: 1 targetCPU: '' targetMemory: '' customLivenessProbe: {} customReadinessProbe: {} customStartupProbe: {} fullnameOverride: '' heapSize: 128m hostAliases: [] initContainers: [] livenessProbe: enabled: true failureThreshold: 5 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 nodeAffinityPreset: key: '' type: '' values: [] nodeSelector: {} podAffinityPreset: '' podAnnotations: {} podAntiAffinityPreset: '' podLabels: {} priorityClassName: '' readinessProbe: enabled: true failureThreshold: 5 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 replicas: 1 resources: limits: {} requests: cpu: 25m memory: 256Mi schedulerName: '' securityContext: enabled: true fsGroup: 1001 runAsUser: 1001 service: annotations: {} loadBalancerIP: '' nodePort: '' port: 9200 type: ClusterIP serviceAccount: create: false name: '' sidecars: [] startupProbe: enabled: false failureThreshold: 5 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 tolerations: [] topologySpreadConstraints: [] updateStrategy: type: RollingUpdate curator: affinity: {} command: - curator configMaps: action_file_yml: |- --- actions: 1: action: delete_indices description: \"Clean up ES by deleting old indices\" options: timeout_override: continue_if_exception: False disable_action: False ignore_empty_list: True filters: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 90 field: stats_result: epoch: exclude: False config_yml: |- --- client: hosts: - {{ template \"elasticsearch.coordinating.fullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }} port: {{ .Values.coordinating.service.port }} # url_prefix: # use_ssl: True # certificate: # client_cert: # client_key: # ssl_no_validate: True # http_auth: # timeout: 30 # master_only: False # logging: # loglevel: INFO # logfile: # logformat: default # blacklist: ['elasticsearch', 'urllib3'] cronjob: annotations: {} concurrencyPolicy: '' failedJobsHistoryLimit: '' jobRestartPolicy: Never schedule: 0 1 * * * successfulJobsHistoryLimit: '' dryrun: false enabled: false env: {} extraInitContainers: [] extraVolumeMounts: [] extraVolumes: [] hooks: install: false upgrade: false image: pullPolicy: IfNotPresent pullSecrets: [] registry: docker.io repository: bitnami/elasticsearch-curator tag: 5.8.4-debian-10-r179 initContainers: [] name: curator nodeAffinityPreset: key: '' type: '' values: [] nodeSelector: {} podAffinityPreset: '' podAnnotations: {} podAntiAffinityPreset: '' podLabels: {} priorityClassName: '' psp: create: fals","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:1","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Kibana Next, we will deploy Kibana to view the logs in Elastic. Login to the Rancher Interface. Go to Apps and Marketplace, Charts. Select only bitnami from the chart repo options. Click on kibana menu item and click Install. Update the following and click Next: Namespace: logging-system Name: kibana-logging Customize Helm options before install: check Replace all the YAML with the below code: Update kibanaPassword: with the password your used in the Elasticsearch deployment. Update ingress.hostname: with the FQDN that you added to DNS for pass through ingress. affinity: {} configuration: server: basePath: '' rewriteBasePath: false configurationCM: '' containerPort: 5601 elasticsearch: hosts: [elasticsearch-logging-master.logging-system] port: 9200 security: auth: enabled: true existingSecret: '' kibanaPassword: 'lockitdown' kibanaUsername: elastic tls: enabled: true existingSecret: 'elasticsearch-logging-master-crt' passwordsSecret: '' truststorePassword: '' usePemCerts: true verificationMode: certificate extraConfiguration: {} extraDeploy: [] extraEnvVars: [] extraEnvVarsCM: '' extraEnvVarsSecret: '' extraVolumeMounts: [] extraVolumes: [] forceInitScripts: false fullnameOverride: '' global: imagePullSecrets: [] imageRegistry: '' storageClass: '' hostAliases: [] image: pullPolicy: IfNotPresent pullSecrets: [] registry: docker.io repository: bitnami/kibana tag: 7.15.2-debian-10-r0 ingress: annotations: {} apiVersion: '' enabled: true extraHosts: [] extraPaths: [] extraTls: [] hostname: kibana.apps.dsodev.net path: / pathType: ImplementationSpecific secrets: [] tls: true initContainers: [] initScriptsCM: '' initScriptsSecret: '' kubeVersion: '' livenessProbe: enabled: true failureThreshold: 6 initialDelaySeconds: 120 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 metrics: enabled: false service: annotations: prometheus.io/path: _prometheus/metrics prometheus.io/port: '80' prometheus.io/scrape: 'true' serviceMonitor: enabled: false interval: '' namespace: '' scrapeTimeout: '' selector: {} nameOverride: '' nodeAffinityPreset: key: '' type: '' values: [] nodeSelector: {} persistence: accessMode: ReadWriteOnce enabled: true existingClaim: '' size: 10Gi storageClass: '' plugins: [] podAffinityPreset: '' podAnnotations: {} podAntiAffinityPreset: soft podLabels: {} readinessProbe: enabled: true failureThreshold: 6 initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 replicaCount: 1 resources: limits: {} requests: {} savedObjects: configmap: '' urls: [] schedulerName: '' securityContext: enabled: true fsGroup: 1001 runAsNonRoot: true runAsUser: 1001 service: annotations: {} externalTrafficPolicy: Cluster extraPorts: [] labels: {} loadBalancerIP: '' nodePort: '' port: 5601 type: ClusterIP serviceAccount: annotations: {} create: true name: '' sidecars: [] tls: autoGenerated: false enabled: false existingSecret: '' keyPassword: '' keystorePassword: '' passwordsSecret: '' usePemCerts: false tolerations: [] updateStrategy: type: RollingUpdate volumePermissions: enabled: false image: pullPolicy: IfNotPresent pullSecrets: [] registry: docker.io repository: bitnami/bitnami-shell tag: 10-debian-10-r248 resources: {} Click Next and click Install. Verify that everthing is up by running the following command: kubectl get all -n logging-system ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:2","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Fluent-bit Next we will install Fluent-bit to actually ship the logs out of Kubernetes and into Elasticsearch. Login to the Rancher Interface. Go to Apps and Marketplace, Repositories. Click Create and add the following then click Create again: Name: fluent Target: Git Git Repo URL: https://github.com/fluent/helm-charts Git Branch: main Click on fluent-bit menu item and click Install. Update the following and click Next: Namespace: logging-system Name: fluentbit-logging Customize Helm options before install: check Replace all the YAML with the below code. Update HTTP_Passwd with the password your used in the Elasticsearch deployment. # Default values for fluent-bit. # kind -- DaemonSet or Deployment kind: DaemonSet # replicaCount -- Only applicable if kind=Deployment replicaCount: 1 image: repository: fluent/fluent-bit # Overrides the image tag whose default is {{ .Chart.AppVersion }} tag: \"\" pullPolicy: Always testFramework: image: repository: busybox pullPolicy: Always tag: latest imagePullSecrets: [] nameOverride: \"\" fullnameOverride: \"\" serviceAccount: create: true annotations: {} name: rbac: create: true nodeAccess: false podSecurityPolicy: create: false annotations: {} podSecurityContext: {} # fsGroup: 2000 hostNetwork: false dnsPolicy: ClusterFirst dnsConfig: {} # nameservers: # - 1.2.3.4 # searches: # - ns1.svc.cluster-domain.example # - my.dns.search.suffix # options: # - name: ndots # value: \"2\" # - name: edns0 hostAliases: [] # - ip: \"1.2.3.4\" # hostnames: # - \"foo.local\" # - \"bar.local\" securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP port: 2020 labels: {} # nodePort: 30020 annotations: {} # prometheus.io/path: \"/api/v1/metrics/prometheus\" # prometheus.io/port: \"2020\" # prometheus.io/scrape: \"true\" serviceMonitor: enabled: false # namespace: monitoring # interval: 10s # scrapeTimeout: 10s # jobLabel: fluentbit # selector: # prometheus: my-prometheus # ## metric relabel configs to apply to samples before ingestion. # ## # metricRelabelings: # - sourceLabels: [__meta_kubernetes_service_label_cluster] # targetLabel: cluster # regex: (.*) # replacement: ${1} # action: replace # ## relabel configs to apply to samples after ingestion. # ## # relabelings: # - sourceLabels: [__meta_kubernetes_pod_node_name] # separator: ; # regex: ^(.*)$ # targetLabel: nodename # replacement: $1 # action: replace prometheusRule: enabled: false # namespace: \"\" # additionnalLabels: {} # rules: # - alert: NoOutputBytesProcessed # expr: rate(fluentbit_output_proc_bytes_total[5m]) == 0 # annotations: # message: | # Fluent Bit instance {{ $labels.instance }}'s output plugin {{ $labels.name }} has not processed any # bytes for at least 15 minutes. # summary: No Output Bytes Processed # for: 15m # labels: # severity: critical dashboards: enabled: false labelKey: grafana_dashboard annotations: {} livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: /api/v1/health port: http resources: {} # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi nodeSelector: {} tolerations: [] affinity: {} labels: {} annotations: {} podAnnotations: {} podLabels: {} priorityClassName: \"\" env: [] envFrom: [] extraContainers: [] # - name: do-something # image: busybox # command: ['do', 'something'] extraPorts: [] # - port: 5170 # containerPort: 5170 # protocol: TCP # name: tcp # nodePort: 30517 extraVolumes: [] extraVolumeMounts: [] updateStrategy: {} # type: RollingUpdate # rollingUpdate: # maxUnavailable: 1 # Make use of a pre-defined configmap instead of the one templated here existingConfigMap: \"\" networkPolicy: enabled: false # ingress: # from: [] luaScripts: {} ## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/configuration-file config: service: | [SERVICE] Daemon Off Flush 1 Log_Level {{ .Values.logLevel }} Parsers_File parsers.conf Parsers_File custom_parsers.conf HTTP_Server On HTTP_Liste","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:3","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Accessing Logs With Kibana Lastly, we need to login to Kibana, add the search index pattern, and start viewing the logs. In a browser, navigate to the FQDN you defined for your Kibana instance. eg: https://kibana.apps.dsodev.net. Login using: Username: elastic Password: the password your used in the Elasticsearch deployment. Click the burger menu in the upper left and select Stack Management at the bottom. Under Kibana, select Index Patterns. Click Create Index Pattern. Enter the following details and click Create Index Pattern: Name: logstash* Timestamp: @timestamp Click the burger menu in the upper left and select Discover. You should see logs from all pods in your cluster flowing into this interface. Kibana UI ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:4","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Closing Based on the instructions in the “Single Node RKE2” blogs, you should now have a solid foundation cluster for running, monitoring, and logging workloads on Kubernetes. Deploying RKE2 or K3s as a highly available, multi-node, production-ready setup involves quite a few more steps. But this guide hopefully gives you the starting point and confidence to go start building that out for yourself or your organization. If you have any questions or would like AlphaBravo’s assistance in building production-grade Kubernetes clusters, please reach out to us at info@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:9:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Instructions on deploying K3s, Helm, cert-manager and Rancher on a single node Hetzner server.","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Single-Node Kubernetes Series Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1) Powerful Single Node K3S on Hetzner for CHEAP! (Part 1a) Powerful Single Node K3s on Hetzner for CHEAP! (Part 2) ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:1:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"The K3s Cluster No, your eyes do not deceive you. This is a very similarly named blog to our Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1). Some folks don’t want to deploy RKE2, and we don’t want you to feel left out. In this blog we are going to deploy K3s in place of RKE2. If you want to see why we chose Hetzner for this specific deployment, go check out the previous blog. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:2:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Why K3s K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. There are a few key differences (as of writing) between K3s and RKE2. K3s supports ARM processors (like Raspberry Pis). K3s is not as focused on FIPS, CIS and other compliance standards as RKE2 K3s deploys with LocalStorage enabled for Persistent Volumes K3s deploys with Traefik ingress controller instead of NGINX ingress controller ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:3:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Deploying a VM IMPORTANT Make sure you keep your SSH keys safe, lock the system down with appropriate firewall rules, and keep the OS updated. So, let’s get started. If you don’t already have a Hetzner account, go sign up here: Hetzner Signup* Once you are signed up: Log in to Hetzner Cloud Click on Default project, or create a new project if you like Click Add Server and select the following options Location: Ashburn, VA (or your desired region) Image: Ubuntu 20.04 Type: Standard and CPX51 Volume: None Network: Create a new one if you like Firewalls: Create a new one that allows all access from your IP and only 80/443 from All IPs SSH Key: Create a new SSH key to use or use an existing one Name: K3s-Dev Click Create \u0026 Buy Now This will provision this new server and provide you and external IP Address that you can use to connect to the node. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:4:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Install K3S Kubernetes Once the node is provisioned, get the IP and login to the cluster using the SSH key you configured. In a terminal, run ssh -i /path/to/key/id_rsa root@YOURSERVERIP Update the OS apt update \u0026\u0026 apt upgrade -y Install K3s: curl -sfL https://get.k3s.io | sh - Export the kubeconfig file. We can use this file to connect from our local machine using the kubectl command line or Lens mkdir ~/.kube cp /etc/rancher/k3s/k3s.yaml ~/.kube/config Run kubectl get nodes to verify that the RKE2 node is up and running root@k3s-dev:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-dev Ready control-plane,master 4m34s v1.21.5+k3s2 You now have a single node K3s cluster up and running. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:5:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Configure DNS (Optional) If you would like the cluster to have a DNS name and be able to pull valid Let’s Encrypt keys, you will need to point some DNS names to your cluster. Add the following to you public DNS records for your desired domain. (We use Cloudflare at AB. Reach out if you have questions.) rancher.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach the Rancher interface we configure in the next steps) *.apps.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach apps running in Kubernetes via NGINX Ingress) ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:6:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Install Helm We will use Helm on the Hetzner machine to deploy Rancher (and other applications if you want). Run the following commands to install Helm: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:7:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Deploy Rancher We can now deploy Rancher 2.6 on the cluster. We will use Rancher as the Management UI for the cluster. Official documentation: https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/ Add the Rancher Stable Helm Repo helm repo add rancher-stable https://releases.rancher.com/server-charts/stable Create a namespace for Rancher kubectl create namespace cattle-system Install cert-manager for management of SSL certificates in cluster # Install cert-manager CRDs kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 Install Rancher using Helm. This assumes you have configured DNS to get a valid certificate. helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.YOURDOMAIN.tld \\ --set bootstrapPassword=SUPERSECRETADMINPASSWORD Verify rancher deployed successfully kubectl -n cattle-system rollout status deploy/rancher We need to allow Traefik to communicate on 80/433 on our host. To do that, create a file named traefik.yml and add the following. apiVersion: v1 kind: Service metadata: name: traefik namespace: kube-system spec: type: LoadBalancer Apply the the file using kubectl kubectl apply -f traefik.yml Once Rancher has been fully deployed, visit https://rancher.YOURDOMAIN.tld in your browser to login to your Rancher UI and interact with your cluster. Now you should have a fully functioning K3s Single Node Cluster with Rancher Management Interface installed. Rancher UI on K3s ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:8:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Closing Based on the instructions above, you should now have a powerful K3s Kubernetes cluster you can use for testing and development work. In the next part of this series, we will go back to RKE2 and install Monitoring and Longhorn for persistent volumes in the cluster. If you have any questions, please reach out to us at devops@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us Note: Some links included here are affiliate links. If you use the link to sign up, AB may get a small referral credit on our account. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:9:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Instructions on deploying RKE2, Helm, cert-manager and Rancher on a single node Hetzner server.","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Single-Node Kubernetes Series Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1) Powerful Single Node K3S on Hetzner for CHEAP! (Part 1a) Powerful Single Node K3s on Hetzner for CHEAP! (Part 2) ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:1:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"The RKE2 Cluster In this blog, we will explain how to deploy a powerful, single node RKE2 cluster on Hetzner. It will provide the following resources. Single Node RKE2 Kubernetes Cluster NGINX Ingress into your cluster Rancher Multi Cluster Manager Web UI to manage your cluster ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:2:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Why Hetzner? Want an easy, powerful, single node, publically accessible Kubernetes cluster? Well, do we have a deal for you! (said in my best Billy Mays voice.) For this, you can use any cloud service. We are recommending Hetzner because they are a reliable and reputable company with many of years of experience operating in the EU and they just opened a data center in US East (Northern Virgina). For the below specs, just look at the price difference. Hetzner: 16 vCPU (AMD) 32GB RAM 360GB SSD 20TB Traffic Hetnzer Price: 49.90/mo Euro ($57.00/mo USD) DigitalOcean: 16 vCPU (Intel) 32GB RAM 50GB SSD 7TB Traffic DigitalOcean Price: $320/mo USD That is a SIGNIFICANT difference in price vs specs. You could run a 5 node cluster in Hetzner for the same price as a single Digital Ocean node. This makes Hetzner ideal for development environments. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:3:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Why RKE2 We will be using RKE2 by Rancher. RKE2 is like K3s in that it is a CNCF certified, single binary Kubernetes installation. RKE2, also known as RKE Government, is Rancher’s next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector. To meet these goals, RKE2 does the following: Provides defaults and configuration options that allow clusters to pass the CIS Kubernetes Benchmark v1.5 or v1.6with minimal operator intervention Enables FIPS 140-2 compliance Regularly scans components for CVEs using trivy in our build pipeline ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:4:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"How is this different from RKE or K3s? RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. Importantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:4:1","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Deploying a VM IMPORTANT Make sure you keep your SSH keys safe, lock the system down with appropriate firewall rules, and keep the OS updated. So, let’s get started. If you don’t already have a Hetzner account, go sign up here: Hetzner Signup* Once you are signed up: Log in to Hetzner Cloud Click on Default project, or create a new project if you like Click Add Server and select the following options Location: Ashburn, VA (or your desired region) Image: Ubuntu 20.04 Type: Standard and CPX51 Volume: None Network: Create a new one if you like Firewalls: Create a new one that allows all access from your IP and only 80/443 from All IPs SSH Key: Create a new SSH key to use or use an existing one Name: RKE2-Dev Click Create \u0026 Buy Now This will provision this new server and provide you and external IP Address that you can use to connect to the node. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:5:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Install RKE2 Kubernetes Once the node is provisioned, get the IP and login to the cluster using the SSH key you configured. In a terminal, run ssh -i /path/to/key/id_rsa root@YOURSERVERIP Update the OS apt update \u0026\u0026 apt upgrade -y Install RKE2: curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=v1.21.6+rke2r1 sh - systemctl enable --now rke2-server.service export PATH=\"/var/lib/rancher/rke2/bin:$PATH\" Export the kubeconfig file. We can use this file to connect from our local machine using the kubectl command line or Lens mkdir ~/.kube cp /etc/rancher/rke2/rke2.yaml ~/.kube/config Run kubectl get nodes to verify that the RKE2 node is up and running root@rke2-test:~# kubectl get nodes NAME STATUS ROLES AGE VERSION rke2-dev Ready control-plane,etcd,master 12d v1.21.6+rke2r1 You now have a single node RKE2 cluster up and running. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:6:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Configure DNS (Optional) If you would like the cluster to have a DNS name and be able to pull valid Let’s Encrypt keys, you will need to point some DNS names to your cluster. Add the following to you public DNS records for your desired domain. (We use Cloudflare at AB. Reach out if you have questions.) rancher.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach the Rancher interface we configure in the next steps) *.apps.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach apps running in Kubernetes via NGINX Ingress) ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:7:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Install Helm We will use Helm on the Hetzner machine to deploy Rancher (and other applications if you want). Run the following commands to install Helm: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:8:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Deploy Rancher We can now deploy Rancher 2.6 on the cluster. We will use Rancher as the Management UI for the cluster. Official documentation: https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/ Add the Rancher Stable Helm Repo helm repo add rancher-stable https://releases.rancher.com/server-charts/stable Create a namespace for Rancher kubectl create namespace cattle-system Install cert-manager for management of SSL certificates in cluster # Install cert-manager CRDs kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 Install Rancher using Helm. helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.YOURDOMAIN.tld \\ --set bootstrapPassword=SUPERSECRETADMINPASSWORD Verify rancher deployed successfully kubectl -n cattle-system rollout status deploy/rancher Once Rancher has been fully deployed, visit https://rancher.YOURDOMAIN.tld in your browser to login to your Rancher UI and interact with your cluster. Now you should have a fully functioning RKE2 Single Node Cluster with Rancher Management Interface installed. Rancher UI on RKE2 ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:9:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Closing Based on the instructions above, you should now have a powerful RKE2 Kubernetes cluster you can use for testing and development work. In Part 2 of this blog series, we will install Monitoring and Longhorn for persistent volumes in the cluster. If you have any questions, please reach out to us at devops@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us Note: Some links included here are affiliate links. If you use the link to sign up, AB may get a small referral credit on our account. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:10:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"A deep dive into why We Chose Nifi Over Databrew","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Why AlphaBravo Chose Nifi Over Databrew Recently, the AlphaBravo engineering team began working on a new product we’re calling ABScan. Our goal is to provide a platform which uses multiple scanning utilities via a single interface. Early on in our development process, we realized that we would need to take multiple JSON outputs, convert them into CSV and SQL, and merge the results. This would require the use of an ETL to automate the transformation of the data generated so we could create CSV files for reports and SQL data for long term storage. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:1:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is an ETL? ETL stands for “Extract, Transform, and Load”. It is a term used when dealing with data, data warehousing, and data analytics. It is a means of bringing in data from multiple sources into a centralized system, such as a database. It works by performing the following work: Extract: Collect data from an originating source, such as one or more scanning applications. Transform: Manipulate the data by merging, deduplication, formatting. Load: Using the transformed data sources, load the information into a file or database. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:2:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is AWS Gule Databrew? When we first began working on the project, we explored ETL platforms which could get us to market as quickly as possible. Given our regular use of the Amazon Web Services (AWS) platform, we reviewed the options available and found Glue Databrew. Glue Databrew is a visual data preparation tool which allows for easy cleaning and normalization of data in preparation for use with analytics and machine learning. The engineering team discovered we could easily upload a dataset, create repeatable recipes for our ETL purposes, and save our results to a database or S3 bucket. The stand-out feature for Databrew is it’s easy to use interface and time-to-market, in addition to easily integrating into AWS platforms and services. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:3:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is Apache Nifi? During our discovery phase, we also researched Apache’s Nifi. Much like AWS Glue Databrew, Apache Nifi provides a visual data preparation tool to clean and normalize data. Using a series of processors, Nifi can extract data from multiple sources, create multiple workflows for ETL purposes, and save the results to a database or S3 bucket. The stand-out feature for Nifi, as compared to AWS, is its ability to allow AlphaBravo to be platform agnostic, whereas Databrew is hard wired into the AWS ecosystem. Nifi also scales exceptionally well and is incredibly fast. What Databrew Does Well During our testing with Databrew, we discovered that the interface is incredibly simple to understand and use. The engineering team were able to create multiple workflow samples within a day, and by the end of day two, we were able to ingest multiple data samples, perform ETL work, merge all of the results into a single dataset, and load the results into an S3 bucket or database. Given that Databrew is a part of the AWS ecosystem, it’s trivial to automate against the platform using multiple toolsets. The AWS CLI works exceptionally well, as does the SDK toolsets, such as the BOTO SDK for Python. Using these tools, we were able to get Databrew completely integrated into our automation pipeline within a few days. Using the UI, the team was able to build easy to use recipes, which we could apply to our sample datasets and return the same result time and time again, as long as the dataset matched the expectations of the recipe. Once we had agreed upon the recipes for our sample datasets, we were able to use these recipes to create jobs with associated datasets to perform the ETL workload. Overall, from start to finish the process rarely took over a few minutes to complete. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:4:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What Databrew Lacks For all of its features, Databrew does have a few weaknesses which made us decide to move off the platform towards something else. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Limits One of the biggest issues we encountered while using Glue Databrew was the inherent limits of the platform. Out of the box, only 10 jobs could be running at any given time. While we could reach out to AWS to increase this limit (and we did), each time we hit a hard limit we would have to reach out to support to resolve the issue. Support would be able to increase the limit, given a good enough reason was provided to do so. Our concern here was we might need to increase the limit significantly at some point but AWS would be slow to handle the increase, or worse, be unwilling to do so. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:1","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Timeouts During our testing process, we encountered a significant number of timeout issues. We realized this was due to the total number of calls being made against the AWS API, and was the result of the engineering team working in parallel and making a non-trivial number of calls to the platform at any given time. We reached out to support to see what we could do to solve these problems, however we were informed that the only way to solve the issue would be to build in logic to throttle API calls over time to prevent the timeout from occurring. This would be a difficult problem to solve over time as we expected to continue to increase API calls as the platform grew, and we were experiencing severe timeouts even during a simple POC. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:2","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Recipe Issues We discovered after running a significant number of datasets that we might have extra information in one random dataset as compared to the rest. Unfortunately, if a recipe is not expecting the extra data, or if data is missing for any reason, the job will fail as the recipe does not understand how to continue. Unfortunately, there is no current method to build in conditions in a given recipe, so the entire job would fail. Further, this would require the team to test a significant number of data samples, determine how any given data sample might fail, and build a specific recipe for the use case. We would then have to build logic to determine what recipes a given dataset would require and build the job around those requirements. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:3","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Job Spin-up Time Glue Databrew is a “just-in-time” platform, meaning the jobs engine stays offline until a call is made to spin up the underlying servers to run a given job. As a result, it can take some time, especially for larger datasets, to complete as each job submitted must wait for the platform to come online. Our average jobs would take a few minutes each to run. Given the limit issue mentioned earlier, this would also become a blocker for other jobs in queue and quickly create a backlog from our automation system. The more people submit data, the longer the last person who created a submission must wait. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:4","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Work Required for Automation Integration To automate against Glue Databrew, a series of actions must be written to perform work against the service. A dataset must be created, referencing a data source such as a database or S3 bucket. A recipe which will work against the dataset must exist or be uploaded. Given that someone can inadvertently delete a recipe from the UI as there’s no way to lock them, it makes sense to upload the recipe as each dataset is created. A means to validate the dataset and recipe have uploaded successfully before a job is submitted. A job submission, which references the dataset and recipe. A method to validate the total number of jobs currently in progress to avoid hitting limits and failure. A timeout function to prevent too many calls from being made against the API, and to back off on API calls over time to prevent the timeout from occurring. A method to verify when a job is complete to determine if the ETL process has completed and the data has output successfully. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:5","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What Nifi Does Well As with Databrew, the Apache Nifi platform has a very simple and easy to use interface. Out of the box, it provides a significant number of processors which are used to handle many kinds of data workflows. Nifi easily integrates with many cloud platforms and service providers, providing simple methods to extract data. It can query and load many different database platforms, message queues, email, APIs, streaming services, logs, ftp, etc. And if you cannot find a processor to fit your needs, you can create your own and import it into the service. Once data has been pulled into the platform, Nifi can easily transform the data and move it along in different workflows. For example, I can pull a JSON document from a message queue, extract the information contained in the file to set one or more attributes, create a flowfile using the attributes, deduplicate and sort the information in the flowfile, merge the contents into a new CSV file, and store the newly created CSV file into an S3 bucket. Did I mention Nifi is fast? It can do all of the work mentioned above in less than a second. In testing, the AphaBravo engineering team has thrown significant amounts of data at Nifi and has been able to retrieve data within a second or two. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:6:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What Nifi Lacks While Nifi is powerful, it does have a few significant limitations as compared to Glue Databrew which had to be considered: ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Self-Hosted Apache Nifi currently, as of the time of this writing, does not have a hosted service available. As such, we had to spin up our own environment. This has a lot of added complexity as compared to Glue Databrew, we have to build out a platform which is powerful enough to process our data without breaking the bank. This also means the engineering team must work on regular updates for the product, as well as manage configurations and securing secrets, which adds additional overhead for the use of the product. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:1","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Scalability Given the Nifi platform is self-hosted, the AlphaBravo team must also take into account the need to scale the platform on-demand. While Glue Databrew can be easily scaled by submitting a support request to increase limits, scale for Nifi must be planned, and preferably, managed automatically. When load on the platform is low, we will not need multiple Nifi nodes. However, if and when we receive regular or bursting traffic, there is a possibility of bottlenecks if we don’t have a means to scale, which can cause ever increasing wait times for users of the platform. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:2","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Always On Unlike Glue Databrew Jobs, Apache Nifi is always on and listening for input. This is, in part, what makes it so fast to process data. However, the downside is the requirements for infrastructure which is always online and must be highly available. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:3","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Lack of Informative Documentation While the Nifi platform has a significant amount of documentation, it’s not very informative. There have been several instances during the POC where a processor was introduced and experienced failures because the documentation was not very clear to the expectations of work. As an example, database integration can be tricky to set up. A user must first find the related processor, for example, “ExecuteSQL”. In order to use this processor, a database connection pooling service must be assigned. If this controller service does not yet exist, it must be created and configured. When setting up the controller service for the database connection pool, you must know the following: The database connection URL: Not only do you need the URL itself, but you must use the exact syntax required by your DBMS. Example: jdbc:mysql://mydatabase:3306 The database driver class name: The name of the driver class to use for connectivity. This is usually provided via the database documentation, and isn’t referenced in the Nifi documentation. Example: com.mysql.jdbc.Driver The location of the database driver on the host node: Generally, the driver is not installed with Nifi. You must download the driver, upload it to the hosted node(s), extract the driver, and store it in a directory which Nifi can access. Example: /opt/nifi/nifi-current/drivers/mysql ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:4","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"So Why Did We Choose Nifi over Databrew? After a significant amount of testing with these two platforms, the AphaBravo engineering team chose Nifi for the following reasons, in no particular order: ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Popularity Nifi is a very, very popular toolset in the ETL community. It is used by many large organizations, as well as government entities. Nifi itself originated from the NSA, based on the NiagraFiles software which was open sourced by the NSA in 2014. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:1","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Age of the Nifi Project Nifi was originally open sourced in 2014, and has an active community. Glue Databrew was released at the end of 2020, but the Glue platform on which it is based was originally released in 2017. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:2","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Platform Agnostic While Glue Databrew is tightly integrated with AWS, Nifi works with all major cloud providers and a host of 3rd party tool sets. This allows the engineering team to host Nifi in many different environments and allows movement from platform to platform without breaking functionality. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:3","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Ability to Scale Nifi scale is only limited to the AlphaBravo team’s capabilities, and is not based on arbitrary limits set by a cloud provider. We could, in theory, scale the platform indefinitely should the need to do so arise. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:4","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Speed Nifi is fast. So fast, in fact, we had to build in timing mechanisms in certain workflows during our POC because the data was collecting so quickly that multiple files were being created inadvertently during data merge processes. We introduced bin timeouts to solve this problem. As a result, the slowest workflow takes approximately 10 seconds to complete end to end. Ed Engelking - AlphaBravo Principal Engineer Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:5","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"It doesn't matter what Kubernetes Engine you use (EKS, AKS, GKE, RKE, K3s, etc), Rancher MCM will help you manage it better.","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"What is Rancher Multi-Cluster Management If you have ventured into container management and Kubernetes for your organization (or even for yourself), you have quickly realized that you need a robust way to manage more than one cluster easily. In fact, you probably want an easier way to manage a single cluster. The official Kubernetes dashboard is… sub-optimal. There are a few other open-source options out there, but they lack in many areas. So what are we supposed to do? # Rancher has entered the chat. This is where the Rancher UI, or Rancher Multi-Cluster Manager (Rancher MCM) comes into play. As engineers responsible for a plethora of technologies, even just drilling down into Kubernetes space we usually have: A local Kubernetes cluster (k3d, k3s on a Raspberry Pi, or similar) 1-2 Dev clusters 1-N Test, QA, etc clusters 1-N Production clusters On each of those clusters we need: Monitoring \u0026 Alerting Logging Backups Direct access to Pod console and logs Simplified deployment capabilities (Helm, etc) into each Quick view into the health of the cluster Much more This presents a problem because our time is already fragmented enough. We really need a unified interface that not only lowers the barrier to entry for less experienced Kubernetes users but also simplifies cluster and app administration for our fragmented attention. ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:1:0","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"What Can Rancher MCM Do? Rancher MCM has numerous capabilities that will make the developer, infrastructure, security and cluster-administrators life much easier. Some of the key features of Rancher MCM are: Certified Distritbution Support: Rancher supports any certified Kubernetes distribution. For on-premises workloads, we offer the RKE. For the public cloud, we support all the major distributions, including EKS, AKS, and GKE. For edge, branch and desktop workloads we offer K3s, a certified lightweight distribution of Kubernetes. Simplified Cluster Operations: Rancher provides simple, consistent cluster operations including provisioning, version management, visibility and diagnostics, monitoring and alerting, and centralized audit. Security, Policy and User Management: Rancher lets you automate processes and applies a consistent set of user access and security policies for all your clusters, no matter where they’re running. Shared Tools and Services: Rancher provides a rich catalog of services for building, deploying and scaling containerized applications, including app packaging, CI/CD, logging, monitoring and service mesh. ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:2:0","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Deploying Rancher MCM There are 2 primary (well-documented) ways that we generally deploy Rancher MCM. ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:0","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Single Host on Docker (Non HA) Docker Install Docker Advanced Options There are many scenarios where we need to stand up a temporary or demo instance of Rancher. While the production, highly available method is to deploy the Helm chart onto a HA Kubernetes cluster, the simplest way to get started is to deploy Rancher as a Docker container. Before you start, make sure you have a modern Linux distro running with a recent version of Docker running. The simple command which includes local persistent storage is: docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /opt/rancher:/var/lib/rancher \\ --privileged \\ rancher/rancher:latest If you are running this on a public server, you can also automatically deploy this and get a valid SSL certificate from Let’s Encrypt using the following command. Make sure that the following is configured: Open port 80 from the internet to your server Configure the \u003cYOUR.DNS.NAME\u003e that you enter below to point to the public IP of your server docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /opt/rancher:/var/lib/rancher \\ --privileged \\ rancher/rancher:latest --acme-domain \u003cYOUR.DNS.NAME\u003e Check out the documentation for additional advanced deployment options for the Rancher Single Node Docker Install Fun Fact: The Docker Install actually deploys a K3d Single Node Cluster under to the hood to run the Rancher MCM UI on top of ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:1","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"HA Deployment of Kubernetes Rancher on K8s Docker Advanced Options Assuming you have a HA (at least 3 master node) Kubernetes cluster up and running already, you can deploy Rancher on top of that to provide you with a HA deployment of you Rancher MCM. In this instance, we will use the Rancher self generated SSL cert, but you can refer to the docs for apply your own SSL certificates. Before you get started you will need: A HA Kubernetes Cluster. Any one will do. Helm installed locally First, you need to install cert-manager in your cluster: # If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 We can verify cert-manager is installed by running: kubectl get pods --namespace cert-manager Next, we can install Rancher: # Add the Rancher Helm repository helm repo add rancher-stable https://releases.rancher.com/server-charts/stable # Create the Rancher Namespace kubectl create namespace cattle-system # Install the Rancher Helm chart helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set bootstrapPassword=admin We can verify our rancher deployment using the following command: kubectl -n cattle-system rollout status deploy/rancher Now you should be able to visit https://rancher.my.org and access the cluster. You can now begin to import additional clusters, select existing clusters to explore and deploy new resources. Here are some videos to get you started with the Rancher 2.6 Interface: Rancher Online Meetup - September 2021 - Introducing SUSE Rancher 2.6 What’s New in Rancher 2.6? Rancher 2.6 UI Walkthrough and Q\u0026A ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:2","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Rancher MCM - Final Words Hopefully, this post provided you with some insight into how valuable the Rancher MCM UI can be to your organization and helped you get started deploying it into your environment. Please reach out to us if you have any questions regarding installing Rancher or if you need any help running your DevSecOps systems. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:3","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Some insight into why AlphaBravo uses Rancher technologies to accelerate DevSecOps initiatives internally and for our customers.","date":"2021-11-15","objectID":"/posts/2021/why-we-use-rancher/","tags":["rancher","kubernetes","rke2","k3s"],"title":"Why We Use Rancher","uri":"/posts/2021/why-we-use-rancher/"},{"categories":null,"content":"Why We Use Rancher Ok, let’s get this out of the way. Yes, we are a Rancher Gold Partner and yes, we provide Rancher Training. We are not those things because we are corpo shills. We partnered with Rancher because we use their products anyway. We use them in production (RKE2 and K3s). We use them on our Raspberry Pi clusters (K3s). We use them in Docker (K3d) And we manage it all with the Rancher UI. So what are some of the reasons why we use Rancher? The products are open source. At AlphaBravo we love Open Source technology and companies that find a way to open source their technology and still provide immense value beyond that open source project. Secure (or at least securable with minimal configuration tweaks). There are varying “out of the box” security settings across the product line, but Rancher does a great job of focusing on differentiating in each area (K3s vs RKE2 for example). Simple to deploy. This is a big one. As engineers who need to do our full-time jobs while also keeping track of the latest technology, it is important to be able to quickly deploy and assess new technologies is critical. With Rancher software and some basic Linux knowledge and some Linux systems around, you can deploy many of their products including a functioning Kubernetes cluster with a SINGLE CLI COMMAND. Let that sink in. Easy to use. When it is 2 AM and you are troubleshooting a production micro-service issue, you definitely need this as a feature. (Shout out to my Pager Duty crew.) I am obviously kidding. We should all be striving to expand our technical capabilities, play well with others, and operate as one big Dev and Ops family. Breaking down the silos and sharing our knowledge is what makes us stronger as a team. Rancher certainly assists in that mission. I mean, have you SEEN the sheer number of open-source products they have? Rancher UI - The Enterprise Kubernetes Management Platform. Rancher is a complete platform for managing Kubernetes clusters wherever you deploy them. RKE - RKE is a CNCF-certified Kubernetes distribution that runs entirely within Docker containers. It solves the common frustration of installation complexity with Kubernetes by removing most host dependencies and presenting a stable path for deployment, upgrades, and rollbacks. RKE2 - RKE2, also known as RKE Government, is Rancher’s next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector. K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. K3os - k3OS is purpose-built to simplify Kubernetes operations in low-resource computing environments. Installs fast. Boots faster. Managed through Kubernetes. k3d - k3d is a lightweight wrapper to run k3s (Rancher Lab’s minimal Kubernetes distribution) in docker. k3d makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes. Longhorn - Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes. Longhorn is free, open-source software. Originally developed by Rancher Labs, it is now being developed as an incubating project of the Cloud Native Computing Foundation. Fleet - Fleet is GitOps at scale. Fleet is designed to manage up to a million clusters. It’s also lightweight enough that it works great for a single cluster too, but it really shines when you get to a large scale. By large scale, we mean either a lot of clusters, a lot of deployments, or a lot of teams in a single organization. Harvester - Harvester is a modern Hyperconverged infrastructure (HCI) solution built for bare metal servers using enterprise-grade open source technologies including Kubernetes, Kubevirt, and Longhorn. Designed for users looking for a cloud-native HCI solution, Harvester is a flexible and affordable offerin","date":"2021-11-15","objectID":"/posts/2021/why-we-use-rancher/:0:0","tags":["rancher","kubernetes","rke2","k3s"],"title":"Why We Use Rancher","uri":"/posts/2021/why-we-use-rancher/"},{"categories":null,"content":"Welcome To The AlphaBravo Engineering Blog","date":"2021-11-15","objectID":"/posts/2021/welcome-to-the-blog/","tags":null,"title":"Welcome To The Blog","uri":"/posts/2021/welcome-to-the-blog/"},{"categories":null,"content":"Welcome To The Blog We are excited to have you here. As technology enthusiasts with a broad range of experiences, we hope to convey some tales, some tech, and some thoughts around the current state of DevSecOps. Our focus is mainly on building robust platforms using open-source software. But don’t be confused if we talk about non-open source software too. We are a small team and we understand very well that sometimes you need a product or tool that “just works”. While we provide technology services in staff augmentation, deployment acceleration, and training, we also are building some cool internal tools that we hope to release to the community soon. Thanks for checking us out and we look forward to hearing your feedback on what we post here. The AB Engineering Team https://alphabravo.io ","date":"2021-11-15","objectID":"/posts/2021/welcome-to-the-blog/:0:0","tags":null,"title":"Welcome To The Blog","uri":"/posts/2021/welcome-to-the-blog/"}]